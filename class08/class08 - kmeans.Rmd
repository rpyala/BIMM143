---
title: "class08"
author: "Reshma Pyala"
date: "2/5/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##K-means clustering
```{r}
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

Use the kmeans() function setting k to 2 and nstart=20

Inspect/print the results

Q. How many points are in each cluster?
Q. What ‘component’ of your result object details
      - cluster size?
      - cluster assignment/membership?
      - cluster center?
      
Plot x colored by the kmeans cluster assignment and
      add cluster centers as blue points

```{r}
km <- kmeans(x, centers = 2, nstart = 20)
km
```

```{r}
km$size
```


Clusster membership assignment vector (i.e. which cluster group my data lies in!)
```{r}
km$cluster
```



```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=16, cex=1.5)
```

```{r}
km$totss
```


##Hierarchial clustering in R

```{r}
# First we need to calculate point (dis)similarity
#   as the Euclidean distance between observations
dist_matrix <- dist(x)
# The hclust() function returns a hierarchical
#  clustering model
hc <- hclust(d = dist_matrix)
# the print method is not so useful here
hc
```

```{r}
View(as.matrix(dist_matrix))
dim(as.matrix(dist_matrix))
```

```{r}
plot(hc)
abline(h=6, col="red")
cutree(hc, h=6) #Cut by height h
```

```{r}
#Can also do this function as a way to divide it into groups without picking a specific height yourself
cutree(hc, k=2)
```

```{r}
# Using different hierarchical clustering methods
d <- dist_matrix
hc.complete <- hclust(d, method="complete")
hc.average  <- hclust(d, method="average")
hc.single   <- hclust(d, method="single")

plot(hc.single)
```

```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
  matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2),   # c1
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
  matrix(c(rnorm(50, mean = 1, sd = 0.3),           # c3
           rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
#         (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col, pch=16, cex=.5)
```

Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters
Q. How does this compare to your known 'col' groups?

```{r}
kmeans()
d <- dist(x)
hc2 <- hclust(d)
plot(hc2)
abline(h=2, col="red")
abline(h=2.5, col="blue")
```

```{r}
gp2 <- cutree(hc2, k=2)
gp3 <- cutree(hc2, k=3)
plot(x, col=gp2)
plot(x, col=gp3)
```

General code for kmeans and hclust
**kmeans(x, centers=2, nstart=20)**
**hclust(dist(x))**

Could also do it all in one run like this:
**plot(hclust(dist(x)))**
but once you run it, you don't have the output saved anywhere so it is better to save things separately to an object and then run more lines of code so that you can use the object name later to run it instead of recalculating for it


##Now trying to do PCA (Principal Component Analysis) in R
```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV",
row.names=1)

head(mydata)
```

```{r}
#Let's do PCA
pca <- prcomp(t(mydata), scale=TRUE)

#See what is returned by the procomp() function
attributes(pca)
pca$names
```

```{r}
##A basic PC1 vs PC2 2D plot
plot(pca$x[,1], pca$x[,2])
```

```{r}
##Variance captured per PC
pca.var <- pca$sdev^2
##Percent variance is often more informative to look at
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```

```{r}
#Now let's plot it
 pca.var <- pca$sdev^2
 pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
 barplot(pca.var.per, main="Scree Plot",
 xlab="Principal Component", ylab="Percent Variation")
```

```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(mydata)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
xlab=paste0("PC1 (", pca.var.per[1], "%)"),
ylab=paste0("PC2 (", pca.var.per[2], "%)"))
```

```{r}
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,xlab=paste0("PC1 (", pca.var.per[1], "%)"),ylab=paste0("PC2 (", pca.var.per[2], "%)"))
## Click to identify which sample is which
identify(pca$x[,1], pca$x[,2], labels=colnames(mydata))
```


##Next hands on section

```{r}
x <- read.csv("UK_foods.csv", row.names =1)
head(x)
#*Came back and added the row.names and head so that the file is read with proper formatting and this way we don't have to adjust it later and complicate things even more*

#How many rows and columns are in your new data frame named *x*? What R functions could you use to answer this queestions?
#I opened up the data saved in the environment panel and saw that there are 17 rows and 5 columns
```

```{r}
## Complete the following code to find out how many rows and columns are in x?
dim(x)
ncol(x)
nrow(x)
#This is reading as 5 columns because it is counting x as a label but that space is meant to be blank - as a cross section between the two variables
```

```{r}
## Preview the first 6 rows
head(x)
#Use head(x) to preview the first couple data points in this data. and use tail(x) to preview the last couple data points in this data.
tail(x)
```

```{r}
# Note how the minus indexing works
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
#Using -1 means to show all the data but column 1
```

```{r}
dim(x)
#This should now have 4 columns since we condensed the data a bit
```

```{r}
x <- read.csv("UK_foods.csv", row.names=1)
head(x)
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

```{r}
pairs(x, col=rainbow(10), pch=16)
```

Let's run pca on this to get some insight
```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x),col=c("orange", "red", "blue", "darkgreen"))
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
## or the second row here...
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```




###Examine the "loadings"

This will help us determine how the original variables (dimensions) contribute to our new PCs

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```


```{r}
## The inbuilt biplot() can be useful for small datasets 
biplot(pca)
```


```{r}
##prcomp function general set up

# prcomp(t(x))

##Main functions from today

#kmeans(x, centers = 2, nstart = 20)
#hclust(dist(x))
#prcomp(t(x))
```




